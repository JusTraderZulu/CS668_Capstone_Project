# DQL Trading System Cloud Deployment Guide

## Overview

This guide extends our DQL trading system to enable deployment across multiple cloud platforms, with specific focus on AWS. The modular design allows for consistent training and evaluation across different computing environments.

## Cloud-Agnostic Architecture

### Key Design Principles

1. **Environment Abstraction Layer**
   - Configuration files that adapt to the execution environment
   - Runtime detection of available resources
   - Dynamic scaling based on workload

2. **Resource Independence**
   - Container-based deployment for consistent environments
   - Storage abstraction for data and model persistence
   - Minimal dependencies on platform-specific APIs

3. **Unified Command Interface**
   - Single entry point script (`dql_trading.py`) works in all environments
   - Environment-specific parameters handled through configuration

## AWS Deployment

### Prerequisites

- AWS account with appropriate permissions
- AWS CLI installed and configured
- Docker installed (for container builds)

### AWS Services Used

1. **Amazon EC2** - Virtual servers for training
   - Training on GPU instances (p3.2xlarge or g4dn.xlarge)
   - Monitoring on t3.small instances
   - Spot instances for cost optimization

2. **Amazon S3** - Storage for datasets and models
   - Centralized data repository
   - Version control for models
   - Accessible from any compute resource

3. **Amazon ECR** - Docker container registry
   - Store custom training images
   - Versioned environments for reproducibility

4. **AWS Batch** (optional) - Job scheduling
   - Queue training jobs
   - Dynamic resource allocation
   - Cost optimization

### Implementation Steps

#### 1. Container Setup

```bash
# Create Dockerfile
cat > Dockerfile << 'EOF'
FROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    curl \
    wget \
    vim \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create directories for data and results
RUN mkdir -p data results logs

# Entry point
ENTRYPOINT ["python", "dql_trading.py"]
EOF

# Build container
docker build -t dql-trading:latest .

# Test container locally
docker run --rm dql-trading:latest --help
```

#### 2. Create S3 Data Storage

```bash
# Create S3 buckets
aws s3 mb s3://dql-trading-data
aws s3 mb s3://dql-trading-models

# Upload datasets
aws s3 cp data/ s3://dql-trading-data/ --recursive

# Create configuration for S3 access
cat > storage_config.py << 'EOF'
import os

# Detect environment
def is_aws():
    """Check if running on AWS"""
    return os.environ.get('AWS_EXECUTION_ENV') is not None

def get_data_path(filename):
    """Get the path to data files based on environment"""
    if is_aws():
        return f"s3://dql-trading-data/{filename}"
    else:
        return os.path.join("data", filename)

def get_model_path(experiment_name, filename):
    """Get the path to model files based on environment"""
    if is_aws():
        return f"s3://dql-trading-models/{experiment_name}/{filename}"
    else:
        return os.path.join("results", experiment_name, filename)
EOF
```

#### 3. Push to ECR

```bash
# Create ECR repository
aws ecr create-repository --repository-name dql-trading

# Get the repository URI
REPO_URI=$(aws ecr describe-repositories --repository-names dql-trading --query 'repositories[0].repositoryUri' --output text)

# Login to ECR
aws ecr get-login-password | docker login --username AWS --password-stdin ${REPO_URI}

# Tag and push
docker tag dql-trading:latest ${REPO_URI}:latest
docker push ${REPO_URI}:latest
```

#### 4. EC2 Training Script

```bash
#!/bin/bash
# aws_train.sh

# Configuration
INSTANCE_TYPE="p3.2xlarge"  # GPU instance
AMI_ID="ami-0c55b159cbfafe1f0"  # Amazon Linux 2 with GPU support
KEY_NAME="your-key-pair"
SECURITY_GROUP="sg-12345"
SUBNET_ID="subnet-12345"
VOLUME_SIZE=100  # GB

# Create instance
INSTANCE_ID=$(aws ec2 run-instances \
    --image-id $AMI_ID \
    --instance-type $INSTANCE_TYPE \
    --key-name $KEY_NAME \
    --security-group-ids $SECURITY_GROUP \
    --subnet-id $SUBNET_ID \
    --block-device-mappings "[{\"DeviceName\":\"/dev/xvda\",\"Ebs\":{\"VolumeSize\":$VOLUME_SIZE,\"DeleteOnTermination\":true}}]" \
    --query 'Instances[0].InstanceId' \
    --output text)

echo "Started instance $INSTANCE_ID"

# Wait for instance to be running
aws ec2 wait instance-running --instance-ids $INSTANCE_ID

# Get public IP
PUBLIC_IP=$(aws ec2 describe-instances \
    --instance-ids $INSTANCE_ID \
    --query 'Reservations[0].Instances[0].PublicIpAddress' \
    --output text)

echo "Instance running at $PUBLIC_IP"

# Create user data script
cat > user_data.sh << EOF
#!/bin/bash
# Install dependencies
yum update -y
amazon-linux-extras install docker -y
systemctl start docker
systemctl enable docker

# Install AWS CLI v2
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
./aws/install

# Login to ECR
aws ecr get-login-password --region ${AWS_REGION} | docker login --username AWS --password-stdin ${REPO_URI}

# Pull container
docker pull ${REPO_URI}:latest

# Run training
docker run -d \
    -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \
    -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \
    -e AWS_DEFAULT_REGION=${AWS_REGION} \
    -e TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN} \
    -e TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID} \
    -e EXPERIMENT_NAME="aws_${INSTANCE_TYPE}_$(date +%Y%m%d_%H%M%S)" \
    ${REPO_URI}:latest full-workflow \
    --agent_type ${AGENT_TYPE} \
    --data_file ${DATA_FILE} \
    --episodes ${EPISODES}

EOF

# Transfer user data to instance
scp -i ${KEY_NAME}.pem user_data.sh ec2-user@${PUBLIC_IP}:~/

# Execute setup script
ssh -i ${KEY_NAME}.pem ec2-user@${PUBLIC_IP} "chmod +x user_data.sh && sudo ./user_data.sh"

echo "Training job started on $INSTANCE_ID"
echo "To check status: ssh -i ${KEY_NAME}.pem ec2-user@${PUBLIC_IP} 'docker logs <container_id>'"
```

#### 5. AWS Batch Alternative (for Multiple Jobs)

```bash
# Create job definition
cat > job-definition.json << EOF
{
  "jobDefinitionName": "DQL-Training",
  "type": "container",
  "containerProperties": {
    "image": "${REPO_URI}:latest",
    "vcpus": 4,
    "memory": 16384,
    "command": ["full-workflow", "--agent_type", "Ref::agent_type", "--data_file", "Ref::data_file", "--experiment_name", "Ref::experiment_name", "--episodes", "Ref::episodes"],
    "environment": [
      {"name": "AWS_DEFAULT_REGION", "value": "${AWS_REGION}"},
      {"name": "TELEGRAM_BOT_TOKEN", "value": "${TELEGRAM_BOT_TOKEN}"},
      {"name": "TELEGRAM_CHAT_ID", "value": "${TELEGRAM_CHAT_ID}"}
    ]
  }
}
EOF

aws batch register-job-definition --cli-input-json file://job-definition.json

# Submit job
aws batch submit-job \
    --job-name "dql-training-$(date +%Y%m%d-%H%M%S)" \
    --job-definition DQL-Training \
    --job-queue your-job-queue \
    --parameters '{"agent_type":"custom","data_file":"full_dataset.csv","experiment_name":"aws_batch_job","episodes":"1000"}'
```

## Google Cloud Integration

Similar to AWS, the system can be deployed on Google Cloud Platform:

1. **Google Compute Engine** for VM instances
2. **Google Cloud Storage** for data/model storage
3. **Google Container Registry** for container images
4. **Google Kubernetes Engine** for orchestration (optional)

The implementation follows the same container-based approach with appropriate GCP API calls.

## Microsoft Azure Integration

For Azure deployment:

1. **Azure VMs** with GPU support
2. **Azure Blob Storage** for data/models
3. **Azure Container Registry**
4. **Azure Batch** for job scheduling

## Local Machine Compatibility

Despite these cloud integrations, the system remains fully compatible with local development:

```bash
# Local training
python dql_trading.py full-workflow \
  --agent_type custom \
  --data_file test_small.csv \
  --experiment_name local_test \
  --episodes 100
```

## Data Synchronization

For seamless transitions between environments:

```bash
# Sync data from local to S3
aws s3 sync data/ s3://dql-trading-data/

# Sync models from S3 to local
aws s3 sync s3://dql-trading-models/experiment_name/ results/experiment_name/

# Sync between cloud providers (e.g., AWS to GCP)
gsutil -m rsync -r s3://dql-trading-models gs://gcp-dql-models
```

## Cost Optimization Strategies

1. **Spot Instances/Preemptible VMs**
   - Up to 90% cost reduction
   - Requires checkpointing for fault tolerance

2. **Auto-scaling**
   - Scale down when not in use
   - Scheduled training during off-peak hours

3. **Resource Right-sizing**
   - Use profiling data to select optimal instance sizes
   - Different instance types for different phases

## Conclusion

This cloud-agnostic approach allows the DQL trading system to be trained and deployed across multiple environments without code changes. By leveraging containers and storage abstraction, you can seamlessly transition between local development, HPC clusters, and various cloud providers based on your needs and budget constraints. 